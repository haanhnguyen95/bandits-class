{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "- read the whole page once before starting to answer questions\n",
    "- keep the first theoretical question (Epsilon first) as a homework (send it to me later)\n",
    "- do the rest during the practical session\n",
    "- you can work in groups (up to 3 students)\n",
    "- send your completed notebook to e.diemert@criteo.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of Epsilon-first (theory)\n",
    "\n",
    "Here is a new policy: Epsilon-first. \n",
    "\n",
    "Algorithm:\n",
    "- parameter: $\\epsilon \\in [0,1]$\n",
    "- for $n \\epsilon$ rounds, play random actions, uniformly\n",
    "- then play $\\operatorname{argmax}_{i} \\bar{X}_i$ until the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions (4 points)\n",
    "Suppose we are in a stochastic game with 2 Bernoulli arms and of duration $n$ turns.\n",
    "- Use Hoeffding's inequality to compute a lower bound on $\\tilde{n}$, the number of samples needed to detect a given $\\Delta = \\mu_i^* - \\mu_i$\n",
    "- write the expected, instantaneous regret $R^-(t)$ at each round before we switch to exploitation\n",
    "- write the expected, instantaneous regret $R^+(t)$ after we switch to exploitation\n",
    "- write the total regret $R_n(\\epsilon, \\delta)$ at the end of the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional reading\n",
    "- A more detailed investigation of this family of policies (aka Explore Than Commit) can be found [here](http://banditalgs.com/2016/09/14/first-steps-explore-then-commit/)\n",
    "- An extension of this idea can be found [here](https://arxiv.org/pdf/1505.00369.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat data/univ-latencies/license.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/univ-latencies/univ-latencies.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Latency of some homepages in the first timesteps\")\n",
    "df.iloc[:10,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,:5].plot(title='Latency over time for a few universities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is transformed so as to come back to a Bernoulli problem. The reward is:\n",
    "- 1 if latency is < threshold\n",
    "- 0 if latency is > threshold\n",
    "\n",
    "for an arbitrary latency threshold.\n",
    "\n",
    "The task boils down to exploring which universities have the lowest latency and exploiting this knowledge to minimize the sum of latencies until the end of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean reward for a few arms\")\n",
    "(df.iloc[:,:5] > 500).astype(int).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(n_arms, n_steps=None, max_latency=None, quantile=.5):\n",
    "    \"\"\"\n",
    "    return a binary dataset.\n",
    "    \n",
    "    n_arms: nb of pullable arms\n",
    "    max_latency: criteria to decide if latency is good/bad (default: inferred from quantile)\n",
    "    quantile: proportion of \"1\" labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('data/univ-latencies/univ-latencies.txt')\n",
    "    if n_arms > df.shape[1]:\n",
    "        raise Exception(\"Please specify max %d arms\" % df.shape[1])\n",
    "    if max_latency is None:\n",
    "        max_latency = df.iloc[:,:n_arms].mean().quantile(quantile)\n",
    "    raw_vector = (df.iloc[:,:n_arms] < max_latency).astype(int)\n",
    "    if n_steps is None:\n",
    "        return raw_vector\n",
    "    if raw_vector.shape[0] > n_steps:\n",
    "        return raw_vector.iloc[:n_steps-1,:]\n",
    "    v = raw_vector.sample(n=n_steps, replace=True, random_state=42)\n",
    "    v.index = np.arange(v.shape[0])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset(5, quantile=.5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "    def pull(self) -> int:\n",
    "        \"\"\"Choose an arm.\"\"\"\n",
    "        raise NotImplemented()\n",
    "    def feedback(self, arm: int, reward):\n",
    "        \"\"\"Process feedback\"\"\"\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_API(bandit_class):\n",
    "    bandit = bandit_class(2)\n",
    "    arm = bandit.pull()\n",
    "    assert type(arm) in (int, np.int, np.int16, np.int32, np.int64), type(arm)\n",
    "    bandit.feedback(0, 0)\n",
    "    bandit.feedback(0, 1)\n",
    "    bandit.feedback(1, 0)\n",
    "    bandit.feedback(1, 1)\n",
    "    print(bandit_class.__name__ + \" sounds correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given Bandit Implems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy(Bandit):\n",
    "    def __init__(self, n_arms, epsilon=.1):\n",
    "        Bandit.__init__(self, n_arms)\n",
    "        self.epsilon = epsilon\n",
    "        self.reward_sum_per_arm = np.zeros(n_arms)\n",
    "        self.actions_per_arm = np.zeros(n_arms)\n",
    "    def pull(self) -> int:\n",
    "        unpulled_arms = [_ for _ in range(self.n_arms) if self.actions_per_arm[_] < 1]\n",
    "        if len(unpulled_arms):\n",
    "            return np.random.choice(unpulled_arms)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            arm = np.random.choice(np.arange(self.n_arms))\n",
    "            return arm\n",
    "        arms_weights = self.reward_sum_per_arm / self.actions_per_arm\n",
    "        return np.argmax(arms_weights)\n",
    "    def feedback(self, arm: int, reward):\n",
    "        self.actions_per_arm[arm] += 1\n",
    "        self.reward_sum_per_arm[arm] += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_API(EpsilonGreedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedRandomBandit(Bandit):\n",
    "    \"\"\"Play a random action according to $\\hat{X_{i,t}}$.\"\"\"\n",
    "    def __init__(self, n_arms):\n",
    "        Bandit.__init__(self, n_arms)\n",
    "        self.reward_sum_per_arm = np.zeros(n_arms)\n",
    "        self.actions_per_arm = np.zeros(n_arms)        \n",
    "    def pull(self) -> int:\n",
    "        unpulled_arms = [_ for _ in range(self.n_arms) if self.actions_per_arm[_] < 10]\n",
    "        if len(unpulled_arms):\n",
    "            return np.random.choice(unpulled_arms)\n",
    "        arms_weights = self.reward_sum_per_arm / self.actions_per_arm\n",
    "        arms_weights = arms_weights / np.sum(arms_weights)\n",
    "        return np.random.choice(self.n_arms, p=arms_weights)\n",
    "    def feedback(self, arm: int, reward):\n",
    "        self.actions_per_arm[arm] = self.actions_per_arm[arm] + 1\n",
    "        self.reward_sum_per_arm[arm] = self.reward_sum_per_arm[arm] + reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_API(WeightedRandomBandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Implement a more elaborate bandit (4 pts)\n",
    "\n",
    "- Choose one algorithm among (UCB, Thompson Sampling and, optionally, EXP3)\n",
    "- Implement it using the Bandit API defined above\n",
    "- Validate it using `test_API()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBandit(Bandit):\n",
    "    def __init__(self, n_arms):\n",
    "        pass\n",
    "    def pull(self) -> int:\n",
    "        return 0\n",
    "    def feedback(self, arm: int, reward):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_API(MyBandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This class is mainly a helper to analyze the behavior of different bandit algorithms.\n",
    "\n",
    "You may extend it if you want additional insight into an algorithm behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment():\n",
    "    \"\"\"Given a Bandit, run an experiment and compute regret.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms, n_steps):\n",
    "        # data\n",
    "        self.n_arms = n_arms\n",
    "        self.data = get_dataset(n_arms, n_steps=n_steps)\n",
    "        self.n_turns = self.data.shape[0]\n",
    "        # regret analysis variables\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.stochastic_regrets = []\n",
    "        # algo analysis variables\n",
    "        self.actions_per_arm = np.zeros(self.n_arms)\n",
    "        self.reward_sum_per_arm = np.zeros(self.n_arms)\n",
    "        self.exploration = []\n",
    "        \n",
    "    def run(self, b: Bandit):\n",
    "        for t in range(self.data.shape[0]):\n",
    "            # bandit action\n",
    "            chosen_arm = b.pull()\n",
    "            reward = self.data.iloc[t, chosen_arm]\n",
    "            b.feedback(chosen_arm, reward)\n",
    "            # regret data gathering\n",
    "            self.actions += [chosen_arm]\n",
    "            self.actions_per_arm[chosen_arm] += 1\n",
    "            self.rewards += [reward]\n",
    "            self.stochastic_regrets += [self.data.iloc[t,self.best_arm] - reward]\n",
    "            # algo data gathering\n",
    "            self.reward_sum_per_arm[chosen_arm] += reward\n",
    "            arms_perceived_performance = self.reward_sum_per_arm / (self.actions_per_arm + 1e-9)\n",
    "            best_looking_arm = np.argmax(arms_perceived_performance)\n",
    "            is_exploration = 0 if np.abs(arms_perceived_performance[best_looking_arm] - arms_perceived_performance[chosen_arm]) < 1e-5 else 1\n",
    "            self.exploration += [is_exploration]\n",
    "        return self\n",
    "            \n",
    "    # Regret metrics\n",
    "    \n",
    "    @property\n",
    "    def best_arm(self):\n",
    "        return np.argmax(self.data.mean().values)\n",
    "    \n",
    "    @property\n",
    "    def worst_arm(self):\n",
    "        return np.argmin(self.data.mean().values)\n",
    "\n",
    "    @property\n",
    "    def delta_perf(self):\n",
    "        return self.data.iloc[:,self.best_arm].mean() - self.data.iloc[:,self.worst_arm].mean()\n",
    "    \n",
    "    @property\n",
    "    def best_arm_cumulated_reward(self):\n",
    "        return self.data.iloc[:,self.best_arm].sum()\n",
    "\n",
    "    @property\n",
    "    def best_arm_mean_reward(self):\n",
    "        return self.data.iloc[:,self.best_arm].mean()\n",
    "    \n",
    "    @property\n",
    "    def cumulated_stochastic_regret(self):\n",
    "        return self.best_arm_cumulated_reward - np.sum(self.reward_sum_per_arm)\n",
    "    \n",
    "    @property\n",
    "    def mean_reward(self):\n",
    "        return np.mean(self.rewards)\n",
    "\n",
    "    @property\n",
    "    def mean_stochastic_regret(self):\n",
    "        return self.best_arm_mean_reward - self.mean_reward\n",
    "\n",
    "    @property\n",
    "    def n_exploration_pulls(self):\n",
    "        return np.sum(self.exploration)\n",
    "\n",
    "    @property\n",
    "    def exploration_pull_ratio(self):\n",
    "        return self.n_exploration_pulls / float(self.n_turns)\n",
    "\n",
    "    def stochastic_regret_summary(self):\n",
    "        return pd.DataFrame(data={\n",
    "                'E[R]': [self.mean_stochastic_regret],\n",
    "                'E[Delta]': [self.delta_perf],\n",
    "                'Rn': [self.cumulated_stochastic_regret],\n",
    "            })\n",
    "    \n",
    "    def cumulated_regret_summary(self):\n",
    "        rng = pd.date_range('1/1/2011', periods=len(self.stochastic_regrets), freq='600s')\n",
    "        ts = pd.DataFrame(data={\n",
    "                'cumulated_stochastic_regret': np.cumsum(self.stochastic_regrets), \n",
    "                'cumulated_exploration': np.cumsum(self.exploration), \n",
    "            }, index=rng)\n",
    "        return ts\n",
    "    \n",
    "    # algorithm metrics\n",
    "    \n",
    "    def arm_plays_summary(self):\n",
    "        true_means = self.data.mean()\n",
    "        comp = pd.DataFrame(pd.concat([\n",
    "                    true_means, \n",
    "                    pd.Series(self.actions_per_arm, index=true_means.index),\n",
    "                    pd.Series(self.reward_sum_per_arm/self.actions_per_arm, index=true_means.index),\n",
    "                ], axis=1).values, columns=['true_mean_reward', 'pull_count', 'observed_mean_reward'])\n",
    "        return comp.sort_values(by='true_mean_reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run one bandit game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms=10\n",
    "n_steps=1000\n",
    "exp = Experiment(n_arms, n_steps).run(EpsilonGreedy(n_arms))\n",
    "exp.stochastic_regret_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (2 points)\n",
    "- run the same with your bandit class to check everything is fine\n",
    "- compare regret with the one of E-greedy\n",
    "- repeat and comment on what you observe; is it expected ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with regret of E-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions (4 points)\n",
    "- run the following code once; what can you say about the optimal $\\epsilon$ ?\n",
    "- run it many times, is it stable ?\n",
    "- change the number of arms; is the optimal $\\epsilon$ constant ?\n",
    "- give an interpretation of the behavior you observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms=10\n",
    "n_steps=1000\n",
    "epsilons = [.001, .01, .1, .2, .3, .5]\n",
    "mean_regrets = [\n",
    "    Experiment(n_arms, n_steps).run(EpsilonGreedy(n_arms, epsilon=eps)).stochastic_regret_summary()['E[R]'].values \n",
    "    for eps in epsilons\n",
    "]\n",
    "\n",
    "plt.semilogx(epsilons, mean_regrets)\n",
    "plt.title('Epsilon-greedy mean stoch. regret as a function of $\\epsilon$')\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('E[R]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing regret in the stochastic case\n",
    "\n",
    "It is important to repeat the experiment to have a good estimate of the variance of the regret as algorithms are randomized themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "n_arms = 10\n",
    "n_repeats = 30\n",
    "n_steps = 1000\n",
    "# Algos to test \n",
    "bandit_classes = [EpsilonGreedy, WeightedRandomBandit]\n",
    "bandit_params = [{'epsilon':.02}, {}]\n",
    "# Metric to evaluate performance\n",
    "stochastic_regret = defaultdict(list)\n",
    "# Run the experiments\n",
    "for _ in range(n_repeats):\n",
    "    for bandit_class, bandit_param in zip(bandit_classes, bandit_params):\n",
    "        exp = Experiment(n_arms, n_steps)\n",
    "        exp.run(bandit_class(n_arms, **bandit_param))\n",
    "        stochastic_regret[bandit_class.__name__] += [exp.mean_stochastic_regret]\n",
    "# Results\n",
    "for bandit_class in bandit_classes:\n",
    "    print(\"E[R] CI for\", bandit_class.__name__, \":\", \n",
    "          np.percentile(stochastic_regret[bandit_class.__name__], [.05, .95]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (2 points)\n",
    "- add your bandit class to the experiment, run it\n",
    "- how is the confidence interval of the expected reward of your algo? is it expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights (optional)\n",
    "\n",
    "You can use the following code to understand the behavior of your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment(n_arms, n_steps).run(EpsilonGreedy(n_arms)).exploration_pull_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment(n_arms, n_steps).run(EpsilonGreedy(n_arms)).arm_plays_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulated Stochastic Regret behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions (4 points)\n",
    "- use the code below to trace cumulated regret of E-greedy; comment using your knowledge of this kind of problems\n",
    "- change the explore/exploit tradeoff; comment your results\n",
    "- change the nber of arms; comment your results\n",
    "- change the nber of steps; comment your results\n",
    "- trace your algorithm regrets; comment your results\n",
    "- compare your algorithm with other bandits; use the known regret bounds to comment on the behavior you observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms=100\n",
    "n_steps=10000\n",
    "exp = Experiment(n_arms, n_steps).run(EpsilonGreedy(n_arms, epsilon=.01))\n",
    "print(exp.stochastic_regret_summary())\n",
    "cumul_data = exp.cumulated_regret_summary()\n",
    "cumul_data.plot(title='Cumulated Regret over Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your comments"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
