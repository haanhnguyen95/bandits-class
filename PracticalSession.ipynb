{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "The goals of this practical session:\n",
    "  1. (warm-up) evaluate empirically 2 given bandit algorithms\n",
    "  1. (core) implement one of the course algorithms: either UCB or Thompson Sampling (optionally EXP3)\n",
    "  1. (core) test it on a practical use case (path selection for network routing)\n",
    "  1. (optional) experiment with the adversarial case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use an open dataset to simulate a network routing problem: \n",
    "- at each round the bandit has the choice between K routes to send his packet\n",
    "- the chosen route reveals its latency when it is used\n",
    "The goal is to minimize the overall latency.\n",
    "\n",
    "To make it easier we will discretize the latency to work on a binary problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cat data/univ-latencies/license.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/univ-latencies/univ-latencies.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Latency of some homepages in the first timesteps\")\n",
    "df.iloc[:10,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,:5].plot(title='Latency over time for a few universities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is transformed so as to come back to a Bernoulli problem. The reward is:\n",
    "- 1 if latency is < threshold\n",
    "- 0 if latency is > threshold\n",
    "\n",
    "for an arbitrary latency threshold.\n",
    "\n",
    "The task boils down to exploring which universities have the lowest latency and exploiting this knowledge to minimize the sum of latencies until the end of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean reward for a few arms\")\n",
    "(df.iloc[:,:5] > 500).astype(int).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(n_arms, n_steps=None, max_latency=None, quantile=.5, shuffle=True):\n",
    "    \"\"\"\n",
    "    return a binary dataset.\n",
    "    \n",
    "    n_arms: nb of pullable arms\n",
    "    max_latency: criteria to decide if latency is good/bad (default: inferred from quantile)\n",
    "    quantile: proportion of \"1\" labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('data/univ-latencies/univ-latencies.txt')\n",
    "    if n_arms > df.shape[1]:\n",
    "        raise Exception(\"Please specify max %d arms\" % df.shape[1])\n",
    "    if max_latency is None:\n",
    "        max_latency = df.iloc[:,:n_arms].mean().quantile(quantile)\n",
    "    raw_vector = (df.iloc[:,:n_arms] < max_latency).astype(int)\n",
    "    if shuffle:\n",
    "        raw_vector = raw_vector.sample(frac=1)\n",
    "    if n_steps is None:\n",
    "        return raw_vector\n",
    "    if raw_vector.shape[0] > n_steps:\n",
    "        return raw_vector.iloc[:n_steps-1,:]\n",
    "    v = raw_vector.sample(n=n_steps, replace=True, random_state=42)\n",
    "    v.index = np.arange(v.shape[0])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarialize(ddf, nb_episods=2):\n",
    "    ddf = ddf.copy()\n",
    "    episod_len = int(ddf.shape[0]/nb_episods)\n",
    "    #print(\"len\", ddf.shape[0], \"epilen\", episod_len)\n",
    "    for _ in range(nb_episods):\n",
    "        start = _*episod_len\n",
    "        end = min((_+1)*episod_len, ddf.shape[0])\n",
    "        #print(\"epi\", _, start, end)\n",
    "        for col in range(ddf.shape[1]):\n",
    "            ddf.iloc[start:end,col] = np.sort(ddf.iloc[start:end,col])\n",
    "            if np.random.randint(0,2) > 0:\n",
    "                ddf.iloc[start:end,col] = np.flip(ddf.iloc[start:end,col])\n",
    "    ddf.index = np.arange(ddf.shape[0])\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset(5, quantile=.5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provided Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "    def pull(self) -> int:\n",
    "        \"\"\"Choose an arm.\"\"\"\n",
    "        raise NotImplemented()\n",
    "    def feedback(self, arm: int, reward):\n",
    "        \"\"\"Process feedback\"\"\"\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_API(bandit_class):\n",
    "    bandit = bandit_class(2)\n",
    "    arm = bandit.pull()\n",
    "    assert type(arm) in (int, np.int, np.int16, np.int32, np.int64), type(arm)\n",
    "    bandit.feedback(0, 0)\n",
    "    bandit.feedback(0, 1)\n",
    "    bandit.feedback(1, 0)\n",
    "    bandit.feedback(1, 1)\n",
    "    print(bandit_class.__name__ + \" sounds correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given Bandit Implems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy(Bandit):\n",
    "    def __init__(self, n_arms, epsilon=.1):\n",
    "        Bandit.__init__(self, n_arms)\n",
    "        self.epsilon = epsilon\n",
    "        self.reward_sum_per_arm = np.zeros(n_arms)\n",
    "        self.actions_per_arm = np.zeros(n_arms)\n",
    "    def pull(self) -> int:\n",
    "        unpulled_arms = [_ for _ in range(self.n_arms) if self.actions_per_arm[_] < 1]\n",
    "        if len(unpulled_arms):\n",
    "            return np.random.choice(unpulled_arms)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            arm = np.random.choice(np.arange(self.n_arms))\n",
    "            return arm\n",
    "        arms_weights = self.reward_sum_per_arm / self.actions_per_arm\n",
    "        return np.argmax(arms_weights)\n",
    "    def feedback(self, arm: int, reward):\n",
    "        self.actions_per_arm[arm] += 1\n",
    "        self.reward_sum_per_arm[arm] += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedRandomBandit(Bandit):\n",
    "    \"\"\"Play a random action according to $\\hat{X_{i,t}}$.\"\"\"\n",
    "    def __init__(self, n_arms):\n",
    "        Bandit.__init__(self, n_arms)\n",
    "        self.reward_sum_per_arm = np.zeros(n_arms)\n",
    "        self.actions_per_arm = np.zeros(n_arms)        \n",
    "    def pull(self) -> int:\n",
    "        unpulled_arms = [_ for _ in range(self.n_arms) if self.actions_per_arm[_] < 10]\n",
    "        if len(unpulled_arms):\n",
    "            return np.random.choice(unpulled_arms)\n",
    "        arms_weights = self.reward_sum_per_arm / self.actions_per_arm\n",
    "        arms_weights = arms_weights / np.sum(arms_weights)\n",
    "        return np.random.choice(self.n_arms, p=arms_weights)\n",
    "    def feedback(self, arm: int, reward):\n",
    "        self.actions_per_arm[arm] = self.actions_per_arm[arm] + 1\n",
    "        self.reward_sum_per_arm[arm] = self.reward_sum_per_arm[arm] + reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "This class is mainly a helper to analyze the behavior of different bandit algorithms.\n",
    "\n",
    "You may extend it if you want additional insight into an algorithm behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment():\n",
    "    \"\"\"Given a Bandit, run an experiment and compute regret.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms:int, n_steps:int, adversarial:int=0):\n",
    "        # data\n",
    "        self.n_arms = n_arms\n",
    "        self.data = get_dataset(n_arms, n_steps=n_steps)\n",
    "        if adversarial > 0:\n",
    "            self.data = adversarialize(self.data, nb_episods=adversarial)\n",
    "        self.n_turns = self.data.shape[0]\n",
    "        # regret analysis variables\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.stochastic_regrets = []\n",
    "        # algo analysis variables\n",
    "        self.actions_per_arm = np.zeros(self.n_arms)\n",
    "        self.reward_sum_per_arm = np.zeros(self.n_arms)\n",
    "        self.exploration = []\n",
    "        \n",
    "    def run(self, b: Bandit):\n",
    "        for t in range(self.data.shape[0]):\n",
    "            # bandit action\n",
    "            chosen_arm = b.pull()\n",
    "            reward = self.data.iloc[t, chosen_arm]\n",
    "            b.feedback(chosen_arm, reward)\n",
    "            # regret data gathering\n",
    "            self.actions += [chosen_arm]\n",
    "            self.actions_per_arm[chosen_arm] += 1\n",
    "            self.rewards += [reward]\n",
    "            self.stochastic_regrets += [self.data.iloc[t,self.best_arm] - reward]\n",
    "            # algo data gathering\n",
    "            self.reward_sum_per_arm[chosen_arm] += reward\n",
    "            arms_perceived_performance = self.reward_sum_per_arm / (self.actions_per_arm + 1e-9)\n",
    "            best_looking_arm = np.argmax(arms_perceived_performance)\n",
    "            is_exploration = 0 if np.abs(arms_perceived_performance[best_looking_arm] - arms_perceived_performance[chosen_arm]) < 1e-5 else 1\n",
    "            self.exploration += [is_exploration]\n",
    "        return self\n",
    "            \n",
    "    # Regret metrics\n",
    "    \n",
    "    @property\n",
    "    def best_arm(self):\n",
    "        return np.argmax(self.data.mean().values)\n",
    "    \n",
    "    @property\n",
    "    def worst_arm(self):\n",
    "        return np.argmin(self.data.mean().values)\n",
    "\n",
    "    @property\n",
    "    def delta_perf(self):\n",
    "        return self.data.iloc[:,self.best_arm].mean() - self.data.iloc[:,self.worst_arm].mean()\n",
    "    \n",
    "    @property\n",
    "    def best_arm_cumulated_reward(self):\n",
    "        return self.data.iloc[:,self.best_arm].sum()\n",
    "\n",
    "    @property\n",
    "    def best_arm_mean_reward(self):\n",
    "        return self.data.iloc[:,self.best_arm].mean()\n",
    "    \n",
    "    @property\n",
    "    def cumulated_stochastic_regret(self):\n",
    "        return self.best_arm_cumulated_reward - np.sum(self.reward_sum_per_arm)\n",
    "    \n",
    "    @property\n",
    "    def mean_reward(self):\n",
    "        return np.mean(self.rewards)\n",
    "\n",
    "    @property\n",
    "    def mean_stochastic_regret(self):\n",
    "        return self.best_arm_mean_reward - self.mean_reward\n",
    "\n",
    "    @property\n",
    "    def n_exploration_pulls(self):\n",
    "        return np.sum(self.exploration)\n",
    "\n",
    "    @property\n",
    "    def exploration_pull_ratio(self):\n",
    "        return self.n_exploration_pulls / float(self.n_turns)\n",
    "\n",
    "    def stochastic_regret_summary(self):\n",
    "        return pd.DataFrame(data={\n",
    "                'E[R]': [self.mean_stochastic_regret],\n",
    "                'E[Delta]': [self.delta_perf],\n",
    "                'Rn': [self.cumulated_stochastic_regret],\n",
    "            })\n",
    "    \n",
    "    def cumulated_regret_summary(self):\n",
    "        rng = pd.date_range('1/1/2011', periods=len(self.stochastic_regrets), freq='600s')\n",
    "        ts = pd.DataFrame(data={\n",
    "                'cumulated_stochastic_regret': np.cumsum(self.stochastic_regrets), \n",
    "                'cumulated_exploration': np.cumsum(self.exploration), \n",
    "            }, index=rng)\n",
    "        return ts\n",
    "    \n",
    "    # algorithm metrics\n",
    "    \n",
    "    def arm_plays_summary(self):\n",
    "        true_means = self.data.mean()\n",
    "        comp = pd.DataFrame(pd.concat([\n",
    "                    true_means, \n",
    "                    pd.Series(self.actions_per_arm, index=true_means.index),\n",
    "                    pd.Series(self.reward_sum_per_arm/self.actions_per_arm, index=true_means.index),\n",
    "                ], axis=1).values, columns=['true_mean_reward', 'pull_count', 'observed_mean_reward'])\n",
    "        return comp.sort_values(by='true_mean_reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [Warm-up] Study of Epsilon Greedy\n",
    "\n",
    "Run the following code once. Examine the cumulated Regret $R_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms=10\n",
    "n_steps=1000\n",
    "exp = Experiment(n_arms, n_steps).run(EpsilonGreedy(n_arms))\n",
    "exp.stochastic_regret_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 parameters of Epsilon Greedy\n",
    "- run the following code once; what can you say about the optimal $\\epsilon$ ?\n",
    "- run it many times, is it stable ?\n",
    "- change the number of arms; is the optimal $\\epsilon$ constant ?\n",
    "- give an interpretation of the behavior you observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms=10\n",
    "n_steps=1000\n",
    "epsilons = [.001, .01, .1, .2, .3, .5]\n",
    "mean_regrets = [\n",
    "    Experiment(n_arms, n_steps).run(EpsilonGreedy(n_arms, epsilon=eps)).stochastic_regret_summary()['E[R]'].values \n",
    "    for eps in epsilons\n",
    "]\n",
    "\n",
    "plt.semilogx(epsilons, mean_regrets)\n",
    "plt.title('Epsilon-greedy mean stoch. regret as a function of $\\epsilon$')\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('E[R]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Comparing regret in the stochastic case\n",
    "\n",
    "It is important to repeat the experiment to have a good estimate of the variance of the regret as algorithms are randomized themselves.\n",
    "\n",
    "The following code runs our 2 bandits: EpsilonGreedy, WeightedRandomBandit and compute a confidence interval on their regret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "n_arms = 10\n",
    "n_repeats = 30\n",
    "n_steps = 1000\n",
    "# Algos to test \n",
    "bandit_classes = [EpsilonGreedy, WeightedRandomBandit]\n",
    "bandit_params = [{'epsilon':.02}, {}]\n",
    "# Metric to evaluate performance\n",
    "stochastic_regret = defaultdict(list)\n",
    "# Run the experiments\n",
    "for _ in range(n_repeats):\n",
    "    for bandit_class, bandit_param in zip(bandit_classes, bandit_params):\n",
    "        exp = Experiment(n_arms, n_steps)\n",
    "        exp.run(bandit_class(n_arms, **bandit_param))\n",
    "        stochastic_regret[bandit_class.__name__] += [exp.mean_stochastic_regret]\n",
    "# Results\n",
    "for bandit_class in bandit_classes:\n",
    "    print(\"E[R] CI for\", bandit_class.__name__, \":\", \n",
    "          np.percentile(stochastic_regret[bandit_class.__name__], [.05, .95]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe ?\n",
    "  - are the confidence intervals tight ?\n",
    "  - which algorithm would you recommend ?\n",
    "  - can you explain the behavior of WeightedRandomBandit ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights (optional)\n",
    "\n",
    "You can use the following code to understand the behavior of your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment(n_arms, n_steps).run(EpsilonGreedy(n_arms)).exploration_pull_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment(n_arms, n_steps).run(EpsilonGreedy(n_arms)).arm_plays_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cumulated Regret behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to trace cumulated regret of E-greedy\n",
    "\n",
    "- comment using your theoretical knowledge on regret bounds - is it good ?\n",
    "- change the explore/exploit tradeoff; comment your results\n",
    "- change the nber of arms; comment your results\n",
    "- change the nber of steps; comment your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms=10\n",
    "n_steps=10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(n_arms, n_steps).run(EpsilonGreedy(n_arms, epsilon=.1))\n",
    "print(exp.stochastic_regret_summary())\n",
    "cumul_data = exp.cumulated_regret_summary()\n",
    "cumul_data['cumulated_stochastic_regret'].plot(title='Cumulated Regret over Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [The Bizz] Implement your algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Your Bandit implementation\n",
    "- Choose one algorithm among (UCB, Thompson Sampling and, optionally, EXP3)\n",
    "- You will implement it using the Bandit API defined above\n",
    "- You can validate it minimally using `test_API()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBandit(Bandit):\n",
    "    def __init__(self, n_arms):\n",
    "        pass\n",
    "    def pull(self) -> int:\n",
    "        return 0\n",
    "    def feedback(self, arm: int, reward):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_API(MyBandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Regret of your algorithm\n",
    "\n",
    "Reuse the code that we saw to study the regret of EpsilonGreedy and apply it to your algorithm\n",
    "\n",
    "- is it better than E-greedy ? is it expected ?\n",
    "- is it stable when you change the number of arms ? of rounds ?\n",
    "- is it stable when you change parameters of your algorithm (if any) ?\n",
    "- based on the results of other students, which algorithm would you recommend ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 (optional) adversarial mode\n",
    "\n",
    "The `Experiment` class has an option to simulate a simple adversarial bandit mode. E.g. `Experiment(n_arms, n_steps=10**5, adversarial=10)`.\n",
    "\n",
    "- Rerun your experiments with this option\n",
    "- Are your previous conclusions robust ?\n",
    "- (optional) look at the adversarial code - is it the worst adversary we can expect ?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
